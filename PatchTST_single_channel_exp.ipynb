{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ETTh1 = pd.read_csv(\"ETDataset/ETT-small/ETTh1.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>HUFL</th>\n",
       "      <th>HULL</th>\n",
       "      <th>MUFL</th>\n",
       "      <th>MULL</th>\n",
       "      <th>LUFL</th>\n",
       "      <th>LULL</th>\n",
       "      <th>OT</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2016-07-01 00:00:00</td>\n",
       "      <td>5.827</td>\n",
       "      <td>2.009</td>\n",
       "      <td>1.599</td>\n",
       "      <td>0.462</td>\n",
       "      <td>4.203</td>\n",
       "      <td>1.340</td>\n",
       "      <td>30.531000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2016-07-01 01:00:00</td>\n",
       "      <td>5.693</td>\n",
       "      <td>2.076</td>\n",
       "      <td>1.492</td>\n",
       "      <td>0.426</td>\n",
       "      <td>4.142</td>\n",
       "      <td>1.371</td>\n",
       "      <td>27.787001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2016-07-01 02:00:00</td>\n",
       "      <td>5.157</td>\n",
       "      <td>1.741</td>\n",
       "      <td>1.279</td>\n",
       "      <td>0.355</td>\n",
       "      <td>3.777</td>\n",
       "      <td>1.218</td>\n",
       "      <td>27.787001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2016-07-01 03:00:00</td>\n",
       "      <td>5.090</td>\n",
       "      <td>1.942</td>\n",
       "      <td>1.279</td>\n",
       "      <td>0.391</td>\n",
       "      <td>3.807</td>\n",
       "      <td>1.279</td>\n",
       "      <td>25.044001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2016-07-01 04:00:00</td>\n",
       "      <td>5.358</td>\n",
       "      <td>1.942</td>\n",
       "      <td>1.492</td>\n",
       "      <td>0.462</td>\n",
       "      <td>3.868</td>\n",
       "      <td>1.279</td>\n",
       "      <td>21.948000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  date   HUFL   HULL   MUFL   MULL   LUFL   LULL         OT\n",
       "0  2016-07-01 00:00:00  5.827  2.009  1.599  0.462  4.203  1.340  30.531000\n",
       "1  2016-07-01 01:00:00  5.693  2.076  1.492  0.426  4.142  1.371  27.787001\n",
       "2  2016-07-01 02:00:00  5.157  1.741  1.279  0.355  3.777  1.218  27.787001\n",
       "3  2016-07-01 03:00:00  5.090  1.942  1.279  0.391  3.807  1.279  25.044001\n",
       "4  2016-07-01 04:00:00  5.358  1.942  1.492  0.462  3.868  1.279  21.948000"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_ETTh1.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(17420, 8)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_ETTh1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ETTDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, dataset=\"ETTh1\", mode=\"train\", scale=True, seq_len=336, pred_len=96):\n",
    "        super().__init__()\n",
    "        df = pd.read_csv(\"ETDataset/ETT-small/{}.csv\".format(dataset))\n",
    "        x_y = df.iloc[:,1:]\n",
    "        time_stamp = df.iloc[:,0]\n",
    "\n",
    "        assert mode in ['train', 'test', 'val']\n",
    "        type_map = {'train': 0, 'val': 1, 'test': 2}\n",
    "        self.set_type = type_map[mode]\n",
    "        \n",
    "        self.seq_len = seq_len\n",
    "        self.pred_len = pred_len\n",
    "\n",
    "        border1s = [0, 12 * 30 * 24 - self.seq_len, 12 * 30 * 24 + 4 * 30 * 24 - self.seq_len]\n",
    "        border2s = [12 * 30 * 24, 12 * 30 * 24 + 4 * 30 * 24, 12 * 30 * 24 + 8 * 30 * 24]\n",
    "        border1 = border1s[self.set_type]\n",
    "        border2 = border2s[self.set_type]\n",
    "\n",
    "        if scale:\n",
    "            train_x_y = x_y.iloc[border1s[0]: border2s[0]]\n",
    "            self.ss = StandardScaler()\n",
    "            self.ss.fit(train_x_y.to_numpy(dtype=np.float32))\n",
    "            x_y = self.ss.transform(x_y.to_numpy(dtype=np.float32))\n",
    "        else:\n",
    "            x_y = x_y.to_numpy(dtype=np.float32)\n",
    "        \n",
    "        time_stamp = time_stamp.to_numpy()     \n",
    "        \n",
    "        self.data_x = x_y[border1: border2, :]\n",
    "        self.data_y = x_y[border1: border2, -1]\n",
    "\n",
    "        self.data_stamp = time_stamp[border1: border2]\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        s_begin = index\n",
    "        s_end = s_begin + self.seq_len\n",
    "        r_begin = s_end\n",
    "        r_end = r_begin + self.pred_len\n",
    "\n",
    "        seq_x = self.data_x[s_begin:s_end]\n",
    "        seq_y = self.data_y[r_begin:r_end]\n",
    "        return seq_x, seq_y\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data_x) - self.seq_len - self.pred_len + 1\n",
    "\n",
    "    def inverse_transform(self, data):\n",
    "        return self.ss.inverse_transform(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#implementation of utils\n",
    "class RevIN(torch.nn.Module):\n",
    "    def __init__(self, num_features: int, eps=1e-5, affine=True, subtract_last=False, target_idx=-1):\n",
    "        super(RevIN, self).__init__()\n",
    "        self.num_features = num_features\n",
    "        self.eps = eps\n",
    "        self.affine = affine\n",
    "        self.subtract_last = subtract_last\n",
    "        self.target_idx = target_idx\n",
    "        if self.affine:\n",
    "            self._init_params()\n",
    "\n",
    "    def forward(self, x, mode):\n",
    "        if mode == \"norm\":\n",
    "            self._get_statistics(x)\n",
    "            x = self._normalize(x)\n",
    "        elif mode == \"denorm\":\n",
    "            x = self._denormalize(x)\n",
    "        else: raise AssertionError\n",
    "        return x\n",
    "\n",
    "    def _init_params(self):\n",
    "        self.affine_weight = torch.nn.Parameter(torch.ones(self.num_features))\n",
    "        self.affine_bias = torch.nn.Parameter(torch.zeros(self.num_features))\n",
    "\n",
    "    def _get_statistics(self, x):\n",
    "        dim2reduce = tuple(range(1, x.ndim-1))\n",
    "        if self.subtract_last:\n",
    "            self.last = x[:,-1,:].unsqueeze(1)\n",
    "        else:\n",
    "            self.mean = torch.mean(x, dim=dim2reduce, keepdim=True).detach()\n",
    "        self.stdev = torch.sqrt(torch.var(x, dim=dim2reduce, keepdim=True, unbiased=False) + self.eps).detach()\n",
    "\n",
    "    def _normalize(self, x):\n",
    "        if self.subtract_last:\n",
    "            x = x - self.last\n",
    "        else:\n",
    "            x = x - self.mean\n",
    "        x = x / self.stdev\n",
    "        if self.affine:\n",
    "            x = x * self.affine_weight\n",
    "            x = x + self.affine_bias\n",
    "        return x\n",
    "\n",
    "    def _denormalize(self, x):\n",
    "        if self.affine:\n",
    "            x = x - self.affine_bias\n",
    "            x = x / (self.affine_weight + self.eps*self.eps)\n",
    "        x = x * self.stdev[:, :, self.target_idx]\n",
    "        if self.subtract_last:\n",
    "            x = x + self.last[:, :, self.target_idx]\n",
    "        else:\n",
    "            x = x + self.mean[:, :, self.target_idx]\n",
    "        return x\n",
    "\n",
    "\n",
    "class Transpose(torch.nn.Module):\n",
    "    def __init__(self, *dims, contiguous=False): \n",
    "        super().__init__()\n",
    "        self.dims, self.contiguous = dims, contiguous\n",
    "    def forward(self, x):\n",
    "        if self.contiguous: \n",
    "            return x.transpose(*self.dims).contiguous()\n",
    "        else: \n",
    "            return x.transpose(*self.dims)\n",
    "\n",
    "\n",
    "def positional_encoding(q_len, d_model):\n",
    "    W_pos = torch.empty((q_len, d_model))\n",
    "    torch.nn.init.uniform_(W_pos, -0.02, 0.02)\n",
    "    return torch.nn.Parameter(W_pos, requires_grad=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Implementation of PatchTST Encorder layer\n",
    "class TSTiEncoder(torch.nn.Module):  #i means channel-independent\n",
    "    def __init__(self, c_in, patch_num, patch_len, max_seq_len=1024,\n",
    "                 n_layers=3, d_model=128, n_heads=16, d_k=None, d_v=None,\n",
    "                 d_ff=256, norm='BatchNorm', attn_dropout=0., dropout=0., store_attn=False,\n",
    "                 key_padding_mask='auto', padding_var=None, attn_mask=None, res_attention=True, pre_norm=False,\n",
    "                 verbose=False, **kwargs):\n",
    "        super().__init__()\n",
    "        self.patch_num = patch_num\n",
    "        self.patch_len = patch_len\n",
    "        # Input encoding\n",
    "        q_len = patch_num\n",
    "        self.W_P = torch.nn.Linear(patch_len, d_model)        # Eq 1: projection of feature vectors onto a d-dim vector space\n",
    "        self.seq_len = q_len\n",
    "        # Positional encoding\n",
    "        self.W_pos = positional_encoding(q_len, d_model)\n",
    "        # Residual dropout\n",
    "        self.dropout = torch.nn.Dropout(dropout)\n",
    "        # Encoder\n",
    "        self.encoder = TSTEncoder(q_len, d_model, n_heads, d_k=d_k, d_v=d_v, d_ff=d_ff, norm=norm, attn_dropout=attn_dropout, dropout=dropout,\n",
    "                                   pre_norm=pre_norm, res_attention=res_attention, n_layers=n_layers, store_attn=store_attn)\n",
    "\n",
    "    def forward(self, x):                                              # x: [bs x nvars x patch_len x patch_num]\n",
    "        \n",
    "        n_vars = x.shape[1]\n",
    "        # Input encoding\n",
    "        x = x.permute(0,1,3,2)                                                   # x: [bs x nvars x patch_num x patch_len]\n",
    "        x = self.W_P(x)                                                          # x: [bs x nvars x patch_num x d_model]\n",
    "\n",
    "        u = torch.reshape(x, (x.shape[0]*x.shape[1],x.shape[2],x.shape[3]))      # u: [bs * nvars x patch_num x d_model]\n",
    "        u = self.dropout(u + self.W_pos)                                         # u: [bs * nvars x patch_num x d_model]\n",
    "\n",
    "        # Encoder\n",
    "        z = self.encoder(u)                                                      # z: [bs * nvars x patch_num x d_model]\n",
    "        z = torch.reshape(z, (-1,n_vars,z.shape[-2],z.shape[-1]))                # z: [bs x nvars x patch_num x d_model]\n",
    "        z = z.permute(0,1,3,2)                                                   # z: [bs x nvars x d_model x patch_num]\n",
    "        \n",
    "        return z       \n",
    "            \n",
    "    \n",
    "# Cell\n",
    "class TSTEncoder(torch.nn.Module):\n",
    "    def __init__(self, q_len, d_model, n_heads, d_k=None, d_v=None, d_ff=None, \n",
    "                        norm='BatchNorm', attn_dropout=0., dropout=0., \n",
    "                        res_attention=False, n_layers=1, pre_norm=False, store_attn=False):\n",
    "        super().__init__()\n",
    "\n",
    "        self.layers = torch.nn.ModuleList([TSTEncoderLayer(q_len, d_model, n_heads=n_heads, d_k=d_k, d_v=d_v, d_ff=d_ff, norm=norm,\n",
    "                                                      attn_dropout=attn_dropout, dropout=dropout,\n",
    "                                                      res_attention=res_attention,\n",
    "                                                      pre_norm=pre_norm, store_attn=store_attn) for i in range(n_layers)])\n",
    "        self.res_attention = res_attention\n",
    "\n",
    "    def forward(self, src, key_padding_mask=None, attn_mask=None):\n",
    "        output = src\n",
    "        scores = None\n",
    "        if self.res_attention:\n",
    "            for mod in self.layers: \n",
    "                output, scores = mod(output, prev=scores, key_padding_mask=key_padding_mask, attn_mask=attn_mask)\n",
    "            return output\n",
    "        else:\n",
    "            for mod in self.layers: \n",
    "                output = mod(output, key_padding_mask=key_padding_mask, attn_mask=attn_mask)\n",
    "            return output\n",
    "\n",
    "\n",
    "\n",
    "class TSTEncoderLayer(torch.nn.Module):\n",
    "    def __init__(self, q_len, d_model, n_heads, d_k=None, d_v=None, d_ff=256, store_attn=False,\n",
    "                 norm='BatchNorm', attn_dropout=0, dropout=0., bias=True, res_attention=False, pre_norm=False):\n",
    "        super().__init__()\n",
    "        assert not d_model%n_heads, f\"d_model ({d_model}) must be divisible by n_heads ({n_heads})\"\n",
    "        d_k = d_model // n_heads if d_k is None else d_k\n",
    "        d_v = d_model // n_heads if d_v is None else d_v\n",
    "\n",
    "        # Multi-Head attention\n",
    "        self.res_attention = res_attention\n",
    "        self.self_attn = _MultiheadAttention(d_model, n_heads, d_k, d_v, attn_dropout=attn_dropout, proj_dropout=dropout, res_attention=res_attention)\n",
    "\n",
    "        # Add & Norm\n",
    "        self.dropout_attn = torch.nn.Dropout(dropout)\n",
    "        if \"batch\" in norm.lower():\n",
    "            self.norm_attn = torch.nn.Sequential(Transpose(1,2), torch.nn.BatchNorm1d(d_model), Transpose(1,2))\n",
    "        else:\n",
    "            self.norm_attn = torch.nn.LayerNorm(d_model)\n",
    "\n",
    "        # Position-wise Feed-Forward\n",
    "        self.ff = torch.nn.Sequential(torch.nn.Linear(d_model, d_ff, bias=bias),\n",
    "                                torch.nn.GELU(),\n",
    "                                torch.nn.Dropout(dropout),\n",
    "                                torch.nn.Linear(d_ff, d_model, bias=bias))\n",
    "\n",
    "        # Add & Norm\n",
    "        self.dropout_ffn = torch.nn.Dropout(dropout)\n",
    "        if \"batch\" in norm.lower():\n",
    "            self.norm_ffn = torch.nn.Sequential(Transpose(1,2), torch.nn.BatchNorm1d(d_model), Transpose(1,2))\n",
    "        else:\n",
    "            self.norm_ffn = torch.nn.LayerNorm(d_model)\n",
    "\n",
    "        self.pre_norm = pre_norm\n",
    "        self.store_attn = store_attn\n",
    "\n",
    "\n",
    "    def forward(self, src, prev=None, key_padding_mask=None, attn_mask=None):\n",
    "\n",
    "        # Multi-Head attention sublayer\n",
    "        if self.pre_norm:\n",
    "            src = self.norm_attn(src)\n",
    "        ## Multi-Head attention\n",
    "        if self.res_attention:\n",
    "            src2, attn, scores = self.self_attn(src, src, src, prev, key_padding_mask=key_padding_mask, attn_mask=attn_mask)\n",
    "        else:\n",
    "            src2, attn = self.self_attn(src, src, src, key_padding_mask=key_padding_mask, attn_mask=attn_mask)\n",
    "        if self.store_attn:\n",
    "            self.attn = attn\n",
    "        ## Add & Norm\n",
    "        src = src + self.dropout_attn(src2) # Add: residual connection with residual dropout\n",
    "        if not self.pre_norm:\n",
    "            src = self.norm_attn(src)\n",
    "\n",
    "        # Feed-forward sublayer\n",
    "        if self.pre_norm:\n",
    "            src = self.norm_ffn(src)\n",
    "        ## Position-wise Feed-Forward\n",
    "        src2 = self.ff(src)\n",
    "        ## Add & Norm\n",
    "        src = src + self.dropout_ffn(src2) # Add: residual connection with residual dropout\n",
    "        if not self.pre_norm:\n",
    "            src = self.norm_ffn(src)\n",
    "\n",
    "        if self.res_attention:\n",
    "            return src, scores\n",
    "        else:\n",
    "            return src\n",
    "        \n",
    "\n",
    "class _MultiheadAttention(torch.nn.Module):\n",
    "    def __init__(self, d_model, n_heads, d_k=None, d_v=None, res_attention=False, attn_dropout=0., proj_dropout=0., qkv_bias=True, lsa=False):\n",
    "        super().__init__()\n",
    "        d_k = d_model // n_heads if d_k is None else d_k\n",
    "        d_v = d_model // n_heads if d_v is None else d_v\n",
    "\n",
    "        self.n_heads, self.d_k, self.d_v = n_heads, d_k, d_v\n",
    "\n",
    "        self.W_Q = torch.nn.Linear(d_model, d_k * n_heads, bias=qkv_bias)\n",
    "        self.W_K = torch.nn.Linear(d_model, d_k * n_heads, bias=qkv_bias)\n",
    "        self.W_V = torch.nn.Linear(d_model, d_v * n_heads, bias=qkv_bias)\n",
    "\n",
    "        # Scaled Dot-Product Attention (multiple heads)\n",
    "        self.res_attention = res_attention\n",
    "        self.sdp_attn = _ScaledDotProductAttention(d_model, n_heads, attn_dropout=attn_dropout, res_attention=self.res_attention, lsa=lsa)\n",
    "\n",
    "        # Poject output\n",
    "        self.to_out = torch.nn.Sequential(torch.nn.Linear(n_heads * d_v, d_model), torch.nn.Dropout(proj_dropout))\n",
    "\n",
    "    def forward(self, Q, K=None, V=None, prev=None,\n",
    "                key_padding_mask=None, attn_mask=None):\n",
    "\n",
    "        bs = Q.size(0)\n",
    "        if K is None: K = Q\n",
    "        if V is None: V = Q\n",
    "\n",
    "        # Linear (+ split in multiple heads)\n",
    "        q_s = self.W_Q(Q).view(bs, -1, self.n_heads, self.d_k).transpose(1,2)       # q_s    : [bs x n_heads x max_q_len x d_k]\n",
    "        k_s = self.W_K(K).view(bs, -1, self.n_heads, self.d_k).permute(0,2,3,1)     # k_s    : [bs x n_heads x d_k x q_len] - transpose(1,2) + transpose(2,3)\n",
    "        v_s = self.W_V(V).view(bs, -1, self.n_heads, self.d_v).transpose(1,2)       # v_s    : [bs x n_heads x q_len x d_v]\n",
    "\n",
    "        # Apply Scaled Dot-Product Attention (multiple heads)\n",
    "        if self.res_attention:\n",
    "            output, attn_weights, attn_scores = self.sdp_attn(q_s, k_s, v_s, prev=prev, key_padding_mask=key_padding_mask, attn_mask=attn_mask)\n",
    "        else:\n",
    "            output, attn_weights = self.sdp_attn(q_s, k_s, v_s, key_padding_mask=key_padding_mask, attn_mask=attn_mask)\n",
    "        # output: [bs x n_heads x q_len x d_v], attn: [bs x n_heads x q_len x q_len], scores: [bs x n_heads x max_q_len x q_len]\n",
    "\n",
    "        # back to the original inputs dimensions\n",
    "        output = output.transpose(1, 2).contiguous().view(bs, -1, self.n_heads * self.d_v) # output: [bs x q_len x n_heads * d_v]\n",
    "        output = self.to_out(output)\n",
    "\n",
    "        if self.res_attention: return output, attn_weights, attn_scores\n",
    "        else: return output, attn_weights\n",
    "\n",
    "\n",
    "class _ScaledDotProductAttention(torch.nn.Module):\n",
    "    def __init__(self, d_model, n_heads, attn_dropout=0., res_attention=False, lsa=False):\n",
    "        super().__init__()\n",
    "        self.attn_dropout = torch.nn.Dropout(attn_dropout)\n",
    "        self.res_attention = res_attention\n",
    "        head_dim = d_model // n_heads\n",
    "        self.scale = torch.nn.Parameter(torch.tensor(head_dim ** -0.5), requires_grad=lsa)\n",
    "        self.lsa = lsa\n",
    "\n",
    "    def forward(self, q, k, v, prev=None, key_padding_mask=None, attn_mask=None):\n",
    "        # Scaled MatMul (q, k) - similarity scores for all pairs of positions in an input sequence\n",
    "        attn_scores = torch.matmul(q, k) * self.scale      # attn_scores : [bs x n_heads x max_q_len x q_len]\n",
    "\n",
    "        # Add pre-softmax attention scores from the previous layer (optional)\n",
    "        if prev is not None: attn_scores = attn_scores + prev\n",
    "\n",
    "        # Attention mask (optional)\n",
    "        if attn_mask is not None:                                     # attn_mask with shape [q_len x seq_len] - only used when q_len == seq_len\n",
    "            if attn_mask.dtype == torch.bool:\n",
    "                attn_scores.masked_fill_(attn_mask, -np.inf)\n",
    "            else:\n",
    "                attn_scores += attn_mask\n",
    "\n",
    "        # Key padding mask (optional)\n",
    "        if key_padding_mask is not None:                              # mask with shape [bs x q_len] (only when max_w_len == q_len)\n",
    "            attn_scores.masked_fill_(key_padding_mask.unsqueeze(1).unsqueeze(2), -np.inf)\n",
    "\n",
    "        # normalize the attention weights\n",
    "        attn_weights = torch.nn.functional.softmax(attn_scores, dim=-1)                 # attn_weights   : [bs x n_heads x max_q_len x q_len]\n",
    "        attn_weights = self.attn_dropout(attn_weights)\n",
    "\n",
    "        # compute the new values given the attention weights\n",
    "        output = torch.matmul(attn_weights, v)                        # output: [bs x n_heads x max_q_len x d_v]\n",
    "\n",
    "        if self.res_attention: return output, attn_weights, attn_scores\n",
    "        else: return output, attn_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Implementation of head layer\n",
    "\n",
    "class Flatten_Head(torch.nn.Module):\n",
    "    def __init__(self, n_vars, nf, target_window, head_dropout=0):\n",
    "        super().__init__()\n",
    "        self.n_vars = n_vars\n",
    "        self.flatten = torch.nn.Flatten(start_dim=-3)\n",
    "        self.linear = torch.nn.Linear(nf * n_vars, target_window) \n",
    "        self.dropout = torch.nn.Dropout(head_dropout)\n",
    "            \n",
    "    def forward(self, x):                                 # x: [bs x nvars x d_model x patch_num]\n",
    "        x = self.flatten(x)                               # x: [bs x nvars * d_model * patch_num]\n",
    "        x = self.linear(x)                                # x: [bs x target_window]\n",
    "        x = self.dropout(x)\n",
    "        return x\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Implementation of PatchTST\n",
    "\n",
    "class PatchTST(torch.nn.Module):\n",
    "    def __init__(self, c_in, context_window, target_window, patch_len, stride, max_seq_len=1024, \n",
    "                 n_layers=3, d_model=16, n_heads=4, d_k=None, d_v=None,\n",
    "                 d_ff=128, attn_dropout=0.0, dropout=0.3, key_padding_mask='auto',\n",
    "                 padding_var=None, attn_mask=None, res_attention=True, pre_norm=False, store_attn=False,\n",
    "                 head_dropout = 0.0, padding_patch = \"end\",\n",
    "                 revin = True, affine = False, subtract_last = False,\n",
    "                 verbose=False, target_idx=-1, **kwargs):\n",
    "        super().__init__()\n",
    "\n",
    "        self.revin = revin\n",
    "        if revin:\n",
    "            self.revin_layer = RevIN(c_in, affine=affine, subtract_last=subtract_last, target_idx=target_idx)\n",
    "\n",
    "        self.patch_len = patch_len\n",
    "        self.stride = stride\n",
    "        self.padding_patch = padding_patch\n",
    "        patch_num = int((context_window - patch_len)/stride + 1)\n",
    "\n",
    "        if padding_patch == \"end\":\n",
    "            self.padding_patch_layer = torch.nn.ReplicationPad1d((0, stride))\n",
    "            patch_num += 1\n",
    "\n",
    "        self.backbone = TSTiEncoder(c_in, patch_num=patch_num, patch_len=patch_len, max_seq_len=max_seq_len,\n",
    "                                n_layers=n_layers, d_model=d_model, n_heads=n_heads, d_k=d_k, d_v=d_v, d_ff=d_ff,\n",
    "                                attn_dropout=attn_dropout, dropout=dropout, key_padding_mask=key_padding_mask, padding_var=padding_var,\n",
    "                                attn_mask=attn_mask, res_attention=res_attention, pre_norm=pre_norm, store_attn=store_attn,\n",
    "                                verbose=verbose, **kwargs)\n",
    "        \n",
    "        self.head_nf = d_model * patch_num\n",
    "        self.n_vars = c_in\n",
    "\n",
    "        self.head = Flatten_Head(self.n_vars, self.head_nf, target_window, head_dropout=head_dropout)\n",
    "        \n",
    "    def forward(self, z):                                                                   # z: [bs x seq_len × nvars]\n",
    "        # instance norm\n",
    "        if self.revin:                                                        \n",
    "            z = self.revin_layer(z, 'norm')\n",
    "            z = z.permute(0,2,1)                                                            # z: [bs x nvars × seq_len]\n",
    "            \n",
    "        # do patching\n",
    "        if self.padding_patch == 'end':\n",
    "            z = self.padding_patch_layer(z)\n",
    "        z = z.unfold(dimension=-1, size=self.patch_len, step=self.stride)                   # z: [bs x nvars x patch_num x patch_len]\n",
    "        z = z.permute(0,1,3,2)                                                              # z: [bs x nvars x patch_len x patch_num]\n",
    "        \n",
    "        # model\n",
    "        z = self.backbone(z)                                                                # z: [bs x nvars x d_model x patch_num]\n",
    "        z = self.head(z)                                                                    # z: [bs x target_window] \n",
    "        \n",
    "        # denorm\n",
    "        if self.revin:                                                        \n",
    "            z = self.revin_layer(z, 'denorm')\n",
    "        return z\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Linear model\n",
    "class Linear(torch.nn.Module):\n",
    "    def __init__(self, c_in, context_window, target_window):\n",
    "        super().__init__()\n",
    "        self.c_in = c_in\n",
    "        self.context_winsoq = context_window\n",
    "        self.target_window = target_window\n",
    "\n",
    "        self.flatten = torch.nn.Flatten(start_dim=-2)\n",
    "\n",
    "        self.linear = torch.nn.Linear(c_in * context_window, target_window)\n",
    "    \n",
    "    def forward(self, x):                   # x: [bs x seq_len × nvars]\n",
    "        x = self.flatten(x)                 # x: [bs x seq_len * nvars]\n",
    "        x = self.linear(x)                  # x: [bs x target_window]\n",
    "        return x\n",
    "    \n",
    "\n",
    "class moving_avg(torch.nn.Module):\n",
    "    def __init__(self, kernel_size, stride):\n",
    "        super().__init__()\n",
    "        self.kernel_size = kernel_size\n",
    "        self.avg = torch.nn.AvgPool1d(kernel_size=kernel_size, stride=stride, padding=0)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # padding on the both ends of time series\n",
    "        front = x[:, 0:1, :].repeat(1, (self.kernel_size - 1) // 2, 1)\n",
    "        end = x[:, -1:, :].repeat(1, (self.kernel_size - 1) // 2, 1)\n",
    "        x = torch.cat([front, x, end], dim=1)\n",
    "        x = self.avg(x.permute(0, 2, 1))\n",
    "        x = x.permute(0, 2, 1)\n",
    "        return x\n",
    "\n",
    "\n",
    "class series_decomp(torch.nn.Module):\n",
    "    def __init__(self, kernel_size):\n",
    "        super().__init__()\n",
    "        self.moving_avg = moving_avg(kernel_size, stride=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        moving_mean = self.moving_avg(x)\n",
    "        res = x - moving_mean\n",
    "        return res, moving_mean\n",
    "\n",
    "class DLinear(torch.nn.Module):\n",
    "    def __init__(self, c_in, context_window, target_window):\n",
    "        super().__init__()\n",
    "        # Decompsition Kernel Size\n",
    "        kernel_size = 25\n",
    "        self.decompsition = series_decomp(kernel_size)\n",
    "        self.flatten_Seasonal = torch.nn.Flatten(start_dim=-2)\n",
    "        self.flatten_Trend = torch.nn.Flatten(start_dim=-2)\n",
    "        \n",
    "        self.Linear_Seasonal = torch.nn.Linear(c_in * context_window, target_window)\n",
    "        self.Linear_Trend = torch.nn.Linear(c_in * context_window, target_window)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x: [Batch, Input length, Channel]\n",
    "        seasonal_init, trend_init = self.decompsition(x)\n",
    "        seasonal_init = self.flatten_Seasonal(x)\n",
    "        trend_init = self.flatten_Trend(x)\n",
    "\n",
    "        seasonal_output = self.Linear_Seasonal(seasonal_init)\n",
    "        trend_output = self.Linear_Trend(trend_init)\n",
    "\n",
    "        x = seasonal_output + trend_output\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Learner:\n",
    "    def __init__(self, model, dataset, batch_size=128, lr=0.0001, epochs=100, target_window=96, d_model=16, adjust_lr=True, adjust_factor=0.001):\n",
    "        self.model = model.to(\"cuda\")\n",
    "        self.batch_size = batch_size\n",
    "        train_dataset = dataset(mode=\"train\")\n",
    "        valid_dataset = dataset(mode=\"val\")\n",
    "        test_dataset = dataset(mode=\"test\")\n",
    "        self.train_datalen = len(train_dataset)\n",
    "        self.valid_datalen = len(valid_dataset)\n",
    "        self.train_dataloader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "        self.valid_dataloader = torch.utils.data.DataLoader(valid_dataset, batch_size=batch_size, shuffle=False)\n",
    "        self.test_dataloader = torch.utils.data.DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "        self.lr=lr\n",
    "        self.optimizer = torch.optim.Adam(self.model.parameters(), lr=self.lr)\n",
    "        self.loss = torch.nn.MSELoss()\n",
    "        self.epochs = epochs\n",
    "        self.target_window=target_window\n",
    "        self.best_weight = self.model.state_dict()\n",
    "        self.d_model=d_model\n",
    "        self.adjust_lr = adjust_lr\n",
    "        self.adjust_factor = adjust_factor\n",
    "    \n",
    "    def adjust_learning_rate(self, steps, warmup_step=300, printout=True):\n",
    "        if steps**(-0.5) < steps * (warmup_step**-1.5):\n",
    "            lr_adjust = (16**-0.5) * (steps**-0.5) * self.adjust_factor\n",
    "        else:\n",
    "            lr_adjust = (16**-0.5) * (steps * (warmup_step**-1.5)) * self.adjust_factor\n",
    "\n",
    "        for param_group in self.optimizer.param_groups:\n",
    "            param_group['lr'] = lr_adjust\n",
    "        if printout: \n",
    "            print('Updating learning rate to {}'.format(lr_adjust))\n",
    "        return \n",
    "\n",
    "    def train(self):\n",
    "        best_valid_loss = np.inf\n",
    "        train_history = []\n",
    "        valid_history = []\n",
    "        train_steps = 1\n",
    "        if self.adjust_lr:\n",
    "            self.adjust_learning_rate(train_steps)\n",
    "        for epoch in range(self.epochs):\n",
    "            #train\n",
    "            self.model.train()\n",
    "            iter_count = 0\n",
    "            total_loss = 0\n",
    "\n",
    "            for train_x, train_y in self.train_dataloader:\n",
    "                train_x = train_x.to(\"cuda\")\n",
    "                train_y = train_y.to(\"cuda\")\n",
    "\n",
    "                pred_y = self.model(train_x)\n",
    "                loss = self.loss(pred_y, train_y)\n",
    "                self.optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                self.optimizer.step()\n",
    "                total_loss += loss.item()\n",
    "                iter_count += 1\n",
    "                train_steps += 1\n",
    "            if self.adjust_lr:\n",
    "                self.adjust_learning_rate(train_steps)\n",
    "\n",
    "            #valid\n",
    "            self.model.eval()\n",
    "            valid_iter_count = 0\n",
    "            valid_total_loss = 0\n",
    "            with torch.no_grad():\n",
    "                for valid_x, valid_y in self.valid_dataloader:\n",
    "                    valid_x = valid_x.to(\"cuda\")\n",
    "                    valid_y = valid_y.to(\"cuda\")\n",
    "                    pred_y = self.model(valid_x)\n",
    "                    loss = self.loss(pred_y, valid_y)\n",
    "                    valid_total_loss += loss.item()\n",
    "                    valid_iter_count += 1\n",
    "            \n",
    "            total_loss /= iter_count\n",
    "            valid_total_loss /= valid_iter_count\n",
    "            print(\"epoch: {} MSE loss: {:.4f} MSE valid loss: {:.4f}\".format(epoch, total_loss, valid_total_loss))\n",
    "            if best_valid_loss >= valid_total_loss:\n",
    "                self.best_weight = self.model.state_dict()\n",
    "                best_valid_loss = valid_total_loss\n",
    "                print(\"Best score! Weights of the model are updated!\")\n",
    "            train_history.append(total_loss)\n",
    "            valid_history.append(valid_total_loss)\n",
    "        return train_history, valid_history\n",
    "\n",
    "    def test(self):\n",
    "        self.model.load_state_dict(self.best_weight)\n",
    "        self.model.eval()\n",
    "        iter_count = 0\n",
    "        total_loss = 0\n",
    "        with torch.no_grad():\n",
    "            for test_x, test_y in self.test_dataloader:\n",
    "                test_x = test_x.to(\"cuda\")\n",
    "                test_y = test_y.to(\"cuda\")\n",
    "                pred_y = self.model(test_x)\n",
    "                loss = self.loss(pred_y, test_y)\n",
    "                total_loss += loss.item()\n",
    "                iter_count += 1\n",
    "        total_loss /= iter_count\n",
    "        print(\"MSE test loss: {:.4f}\".format(total_loss))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "c_in = 7\n",
    "context_window = 336\n",
    "target_window = 96\n",
    "patch_len = 16\n",
    "stride = 8 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "patchtst_model = PatchTST(c_in=c_in, context_window=context_window, target_window=target_window, patch_len=patch_len, stride=stride)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "Linear_model = Linear(c_in=c_in, context_window=context_window, target_window=target_window)\n",
    "DLinear_model = DLinear(c_in=c_in, context_window=context_window, target_window=target_window)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "Linear_learner = Learner(model=Linear_model, dataset=ETTDataset, adjust_lr=True, adjust_factor=0.01)\n",
    "DLinear_learner = Learner(model=DLinear_model, dataset=ETTDataset, adjust_lr=True, adjust_factor=0.01)\n",
    "patchtst_learner = Learner(model=patchtst_model, dataset=ETTDataset, adjust_lr=True, adjust_factor=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updating learning rate to 4.811252243246882e-07\n",
      "Updating learning rate to 3.1754264805429416e-05\n",
      "epoch: 0 MSE loss: 1.2215 MSE valid loss: 0.8030\n",
      "Best score! Weights of the model are updated!\n",
      "Updating learning rate to 6.302740438653415e-05\n",
      "epoch: 1 MSE loss: 0.5945 MSE valid loss: 0.7377\n",
      "Best score! Weights of the model are updated!\n",
      "Updating learning rate to 9.430054396763888e-05\n",
      "epoch: 2 MSE loss: 0.3218 MSE valid loss: 0.5980\n",
      "Best score! Weights of the model are updated!\n",
      "Updating learning rate to 0.00012557368354874362\n",
      "epoch: 3 MSE loss: 0.2473 MSE valid loss: 0.4216\n",
      "Best score! Weights of the model are updated!\n",
      "Updating learning rate to 0.00013846219390542782\n",
      "epoch: 4 MSE loss: 0.2102 MSE valid loss: 0.2953\n",
      "Best score! Weights of the model are updated!\n",
      "Updating learning rate to 0.0001264304343560434\n",
      "epoch: 5 MSE loss: 0.1883 MSE valid loss: 0.2548\n",
      "Best score! Weights of the model are updated!\n",
      "Updating learning rate to 0.00011707322644771175\n",
      "epoch: 6 MSE loss: 0.1757 MSE valid loss: 0.2473\n",
      "Best score! Weights of the model are updated!\n",
      "Updating learning rate to 0.00010952698858458088\n",
      "epoch: 7 MSE loss: 0.1686 MSE valid loss: 0.2475\n",
      "Updating learning rate to 0.00010327404809650345\n",
      "epoch: 8 MSE loss: 0.1628 MSE valid loss: 0.2447\n",
      "Best score! Weights of the model are updated!\n",
      "Updating learning rate to 9.798272520870258e-05\n",
      "epoch: 9 MSE loss: 0.1585 MSE valid loss: 0.2472\n",
      "Updating learning rate to 9.342938659399198e-05\n",
      "epoch: 10 MSE loss: 0.1560 MSE valid loss: 0.2488\n",
      "Updating learning rate to 8.945703337056415e-05\n",
      "epoch: 11 MSE loss: 0.1530 MSE valid loss: 0.2476\n",
      "Updating learning rate to 8.595177052156611e-05\n",
      "epoch: 12 MSE loss: 0.1504 MSE valid loss: 0.2483\n",
      "Updating learning rate to 8.282869524032146e-05\n",
      "epoch: 13 MSE loss: 0.1483 MSE valid loss: 0.2478\n",
      "Updating learning rate to 8.002304995805999e-05\n",
      "epoch: 14 MSE loss: 0.1473 MSE valid loss: 0.2487\n",
      "Updating learning rate to 7.748446592171796e-05\n",
      "epoch: 15 MSE loss: 0.1450 MSE valid loss: 0.2473\n",
      "Updating learning rate to 7.517309741553296e-05\n",
      "epoch: 16 MSE loss: 0.1439 MSE valid loss: 0.2427\n",
      "Best score! Weights of the model are updated!\n",
      "Updating learning rate to 7.305695402335044e-05\n",
      "epoch: 17 MSE loss: 0.1429 MSE valid loss: 0.2428\n",
      "Updating learning rate to 7.111001549857179e-05\n",
      "epoch: 18 MSE loss: 0.1419 MSE valid loss: 0.2477\n",
      "Updating learning rate to 6.931087162517846e-05\n",
      "epoch: 19 MSE loss: 0.1410 MSE valid loss: 0.2441\n",
      "Updating learning rate to 6.76417225936176e-05\n",
      "epoch: 20 MSE loss: 0.1390 MSE valid loss: 0.2468\n",
      "Updating learning rate to 6.608763214317867e-05\n",
      "epoch: 21 MSE loss: 0.1385 MSE valid loss: 0.2434\n",
      "Updating learning rate to 6.46359612493774e-05\n",
      "epoch: 22 MSE loss: 0.1383 MSE valid loss: 0.2445\n",
      "Updating learning rate to 6.327593294406921e-05\n",
      "epoch: 23 MSE loss: 0.1365 MSE valid loss: 0.2411\n",
      "Best score! Weights of the model are updated!\n",
      "Updating learning rate to 6.199829383043036e-05\n",
      "epoch: 24 MSE loss: 0.1363 MSE valid loss: 0.2430\n",
      "Updating learning rate to 6.079504788572469e-05\n",
      "epoch: 25 MSE loss: 0.1347 MSE valid loss: 0.2421\n",
      "Updating learning rate to 5.965924498791846e-05\n",
      "epoch: 26 MSE loss: 0.1352 MSE valid loss: 0.2451\n",
      "Updating learning rate to 5.8584811349126495e-05\n",
      "epoch: 27 MSE loss: 0.1350 MSE valid loss: 0.2434\n",
      "Updating learning rate to 5.7566412382313e-05\n",
      "epoch: 28 MSE loss: 0.1334 MSE valid loss: 0.2426\n",
      "Updating learning rate to 5.659934091583233e-05\n",
      "epoch: 29 MSE loss: 0.1328 MSE valid loss: 0.2433\n",
      "Updating learning rate to 5.567942539842175e-05\n",
      "epoch: 30 MSE loss: 0.1320 MSE valid loss: 0.2428\n",
      "Updating learning rate to 5.4802954002676805e-05\n",
      "epoch: 31 MSE loss: 0.1323 MSE valid loss: 0.2424\n",
      "Updating learning rate to 5.39666114720432e-05\n",
      "epoch: 32 MSE loss: 0.1303 MSE valid loss: 0.2402\n",
      "Best score! Weights of the model are updated!\n",
      "Updating learning rate to 5.316742625738918e-05\n",
      "epoch: 33 MSE loss: 0.1299 MSE valid loss: 0.2394\n",
      "Best score! Weights of the model are updated!\n",
      "Updating learning rate to 5.240272601878982e-05\n",
      "epoch: 34 MSE loss: 0.1299 MSE valid loss: 0.2381\n",
      "Best score! Weights of the model are updated!\n",
      "Updating learning rate to 5.1670099971819504e-05\n",
      "epoch: 35 MSE loss: 0.1296 MSE valid loss: 0.2399\n",
      "Updating learning rate to 5.096736686795811e-05\n",
      "epoch: 36 MSE loss: 0.1284 MSE valid loss: 0.2392\n",
      "Updating learning rate to 5.0292547639160534e-05\n",
      "epoch: 37 MSE loss: 0.1284 MSE valid loss: 0.2413\n",
      "Updating learning rate to 4.96438419243461e-05\n",
      "epoch: 38 MSE loss: 0.1287 MSE valid loss: 0.2371\n",
      "Best score! Weights of the model are updated!\n",
      "Updating learning rate to 4.901960784313725e-05\n",
      "epoch: 39 MSE loss: 0.1280 MSE valid loss: 0.2370\n",
      "Best score! Weights of the model are updated!\n",
      "Updating learning rate to 4.841834449897069e-05\n",
      "epoch: 40 MSE loss: 0.1272 MSE valid loss: 0.2361\n",
      "Best score! Weights of the model are updated!\n",
      "Updating learning rate to 4.783867678672252e-05\n",
      "epoch: 41 MSE loss: 0.1272 MSE valid loss: 0.2407\n",
      "Updating learning rate to 4.727934215451461e-05\n",
      "epoch: 42 MSE loss: 0.1272 MSE valid loss: 0.2373\n",
      "Updating learning rate to 4.6739179029417445e-05\n",
      "epoch: 43 MSE loss: 0.1266 MSE valid loss: 0.2434\n",
      "Updating learning rate to 4.621711666540848e-05\n",
      "epoch: 44 MSE loss: 0.1259 MSE valid loss: 0.2360\n",
      "Best score! Weights of the model are updated!\n",
      "Updating learning rate to 4.571216621155238e-05\n",
      "epoch: 45 MSE loss: 0.1255 MSE valid loss: 0.2388\n",
      "Updating learning rate to 4.522341283077635e-05\n",
      "epoch: 46 MSE loss: 0.1269 MSE valid loss: 0.2374\n",
      "Updating learning rate to 4.4750008726252546e-05\n",
      "epoch: 47 MSE loss: 0.1250 MSE valid loss: 0.2376\n",
      "Updating learning rate to 4.4291166954394496e-05\n",
      "epoch: 48 MSE loss: 0.1250 MSE valid loss: 0.2396\n",
      "Updating learning rate to 4.384615592171157e-05\n",
      "epoch: 49 MSE loss: 0.1244 MSE valid loss: 0.2389\n",
      "Updating learning rate to 4.341429447794924e-05\n",
      "epoch: 50 MSE loss: 0.1240 MSE valid loss: 0.2404\n",
      "Updating learning rate to 4.299494753063186e-05\n",
      "epoch: 51 MSE loss: 0.1243 MSE valid loss: 0.2397\n",
      "Updating learning rate to 4.2587522116770666e-05\n",
      "epoch: 52 MSE loss: 0.1231 MSE valid loss: 0.2369\n",
      "Updating learning rate to 4.219146387646126e-05\n",
      "epoch: 53 MSE loss: 0.1230 MSE valid loss: 0.2376\n",
      "Updating learning rate to 4.1806253880665696e-05\n",
      "epoch: 54 MSE loss: 0.1231 MSE valid loss: 0.2377\n",
      "Updating learning rate to 4.143140577189121e-05\n",
      "epoch: 55 MSE loss: 0.1229 MSE valid loss: 0.2363\n",
      "Updating learning rate to 4.106646318193315e-05\n",
      "epoch: 56 MSE loss: 0.1220 MSE valid loss: 0.2353\n",
      "Best score! Weights of the model are updated!\n",
      "Updating learning rate to 4.071099739550263e-05\n",
      "epoch: 57 MSE loss: 0.1220 MSE valid loss: 0.2384\n",
      "Updating learning rate to 4.0364605232539164e-05\n",
      "epoch: 58 MSE loss: 0.1218 MSE valid loss: 0.2403\n",
      "Updating learning rate to 4.0026907125422175e-05\n",
      "epoch: 59 MSE loss: 0.1220 MSE valid loss: 0.2385\n",
      "Updating learning rate to 3.969754537023144e-05\n",
      "epoch: 60 MSE loss: 0.1215 MSE valid loss: 0.2379\n",
      "Updating learning rate to 3.937618253373847e-05\n",
      "epoch: 61 MSE loss: 0.1216 MSE valid loss: 0.2371\n",
      "Updating learning rate to 3.90625e-05\n",
      "epoch: 62 MSE loss: 0.1222 MSE valid loss: 0.2347\n",
      "Best score! Weights of the model are updated!\n",
      "Updating learning rate to 3.875619664232189e-05\n",
      "epoch: 63 MSE loss: 0.1207 MSE valid loss: 0.2374\n",
      "Updating learning rate to 3.845698760800996e-05\n",
      "epoch: 64 MSE loss: 0.1203 MSE valid loss: 0.2386\n",
      "Updating learning rate to 3.816460320475954e-05\n",
      "epoch: 65 MSE loss: 0.1209 MSE valid loss: 0.2371\n",
      "Updating learning rate to 3.787878787878788e-05\n",
      "epoch: 66 MSE loss: 0.1204 MSE valid loss: 0.2417\n",
      "Updating learning rate to 3.759929927590882e-05\n",
      "epoch: 67 MSE loss: 0.1202 MSE valid loss: 0.2401\n",
      "Updating learning rate to 3.732590737770921e-05\n",
      "epoch: 68 MSE loss: 0.1209 MSE valid loss: 0.2452\n",
      "Updating learning rate to 3.7058393705829335e-05\n",
      "epoch: 69 MSE loss: 0.1194 MSE valid loss: 0.2376\n",
      "Updating learning rate to 3.679655058809149e-05\n",
      "epoch: 70 MSE loss: 0.1195 MSE valid loss: 0.2412\n",
      "Updating learning rate to 3.654018048087443e-05\n",
      "epoch: 71 MSE loss: 0.1188 MSE valid loss: 0.2395\n",
      "Updating learning rate to 3.6289095342709304e-05\n",
      "epoch: 72 MSE loss: 0.1188 MSE valid loss: 0.2375\n",
      "Updating learning rate to 3.604311605458291e-05\n",
      "epoch: 73 MSE loss: 0.1187 MSE valid loss: 0.2421\n",
      "Updating learning rate to 3.58020718828878e-05\n",
      "epoch: 74 MSE loss: 0.1188 MSE valid loss: 0.2387\n",
      "Updating learning rate to 3.5565799981359994e-05\n",
      "epoch: 75 MSE loss: 0.1188 MSE valid loss: 0.2367\n",
      "Updating learning rate to 3.5334144928703015e-05\n",
      "epoch: 76 MSE loss: 0.1187 MSE valid loss: 0.2396\n",
      "Updating learning rate to 3.510695829891526e-05\n",
      "epoch: 77 MSE loss: 0.1186 MSE valid loss: 0.2417\n",
      "Updating learning rate to 3.488409826162172e-05\n",
      "epoch: 78 MSE loss: 0.1174 MSE valid loss: 0.2381\n",
      "Updating learning rate to 3.4665429209964995e-05\n",
      "epoch: 79 MSE loss: 0.1178 MSE valid loss: 0.2402\n",
      "Updating learning rate to 3.445082141383733e-05\n",
      "epoch: 80 MSE loss: 0.1186 MSE valid loss: 0.2406\n",
      "Updating learning rate to 3.424015069643934e-05\n",
      "epoch: 81 MSE loss: 0.1178 MSE valid loss: 0.2418\n",
      "Updating learning rate to 3.4033298132333136e-05\n",
      "epoch: 82 MSE loss: 0.1176 MSE valid loss: 0.2388\n",
      "Updating learning rate to 3.383014976532195e-05\n",
      "epoch: 83 MSE loss: 0.1175 MSE valid loss: 0.2425\n",
      "Updating learning rate to 3.3630596344635906e-05\n",
      "epoch: 84 MSE loss: 0.1171 MSE valid loss: 0.2419\n",
      "Updating learning rate to 3.343453307803635e-05\n",
      "epoch: 85 MSE loss: 0.1180 MSE valid loss: 0.2383\n",
      "Updating learning rate to 3.3241859400571366e-05\n",
      "epoch: 86 MSE loss: 0.1168 MSE valid loss: 0.2396\n",
      "Updating learning rate to 3.305247875782325e-05\n",
      "epoch: 87 MSE loss: 0.1164 MSE valid loss: 0.2385\n",
      "Updating learning rate to 3.286629840258662e-05\n",
      "epoch: 88 MSE loss: 0.1163 MSE valid loss: 0.2398\n",
      "Updating learning rate to 3.268322920400467e-05\n",
      "epoch: 89 MSE loss: 0.1166 MSE valid loss: 0.2415\n",
      "Updating learning rate to 3.250318546827149e-05\n",
      "epoch: 90 MSE loss: 0.1165 MSE valid loss: 0.2399\n",
      "Updating learning rate to 3.232608477008077e-05\n",
      "epoch: 91 MSE loss: 0.1165 MSE valid loss: 0.2410\n",
      "Updating learning rate to 3.21518477940684e-05\n",
      "epoch: 92 MSE loss: 0.1163 MSE valid loss: 0.2447\n",
      "Updating learning rate to 3.198039818555568e-05\n",
      "epoch: 93 MSE loss: 0.1157 MSE valid loss: 0.2407\n",
      "Updating learning rate to 3.181166240995547e-05\n",
      "epoch: 94 MSE loss: 0.1154 MSE valid loss: 0.2415\n",
      "Updating learning rate to 3.1645569620253167e-05\n",
      "epoch: 95 MSE loss: 0.1164 MSE valid loss: 0.2415\n",
      "Updating learning rate to 3.148205153202001e-05\n",
      "epoch: 96 MSE loss: 0.1158 MSE valid loss: 0.2428\n",
      "Updating learning rate to 3.1321042305458135e-05\n",
      "epoch: 97 MSE loss: 0.1152 MSE valid loss: 0.2419\n",
      "Updating learning rate to 3.1162478434014266e-05\n",
      "epoch: 98 MSE loss: 0.1160 MSE valid loss: 0.2450\n",
      "Updating learning rate to 3.100629863913435e-05\n",
      "epoch: 99 MSE loss: 0.1154 MSE valid loss: 0.2444\n",
      "Updating learning rate to 4.811252243246882e-07\n",
      "Updating learning rate to 3.1754264805429416e-05\n",
      "epoch: 0 MSE loss: 1.4931 MSE valid loss: 1.2688\n",
      "Best score! Weights of the model are updated!\n",
      "Updating learning rate to 6.302740438653415e-05\n",
      "epoch: 1 MSE loss: 0.6052 MSE valid loss: 0.8162\n",
      "Best score! Weights of the model are updated!\n",
      "Updating learning rate to 9.430054396763888e-05\n",
      "epoch: 2 MSE loss: 0.3306 MSE valid loss: 0.5338\n",
      "Best score! Weights of the model are updated!\n",
      "Updating learning rate to 0.00012557368354874362\n",
      "epoch: 3 MSE loss: 0.2566 MSE valid loss: 0.3813\n",
      "Best score! Weights of the model are updated!\n",
      "Updating learning rate to 0.00013846219390542782\n",
      "epoch: 4 MSE loss: 0.2172 MSE valid loss: 0.3181\n",
      "Best score! Weights of the model are updated!\n",
      "Updating learning rate to 0.0001264304343560434\n",
      "epoch: 5 MSE loss: 0.1945 MSE valid loss: 0.2988\n",
      "Best score! Weights of the model are updated!\n",
      "Updating learning rate to 0.00011707322644771175\n",
      "epoch: 6 MSE loss: 0.1818 MSE valid loss: 0.2880\n",
      "Best score! Weights of the model are updated!\n",
      "Updating learning rate to 0.00010952698858458088\n",
      "epoch: 7 MSE loss: 0.1726 MSE valid loss: 0.2861\n",
      "Best score! Weights of the model are updated!\n",
      "Updating learning rate to 0.00010327404809650345\n",
      "epoch: 8 MSE loss: 0.1663 MSE valid loss: 0.2772\n",
      "Best score! Weights of the model are updated!\n",
      "Updating learning rate to 9.798272520870258e-05\n",
      "epoch: 9 MSE loss: 0.1612 MSE valid loss: 0.2821\n",
      "Updating learning rate to 9.342938659399198e-05\n",
      "epoch: 10 MSE loss: 0.1583 MSE valid loss: 0.2764\n",
      "Best score! Weights of the model are updated!\n",
      "Updating learning rate to 8.945703337056415e-05\n",
      "epoch: 11 MSE loss: 0.1556 MSE valid loss: 0.2729\n",
      "Best score! Weights of the model are updated!\n",
      "Updating learning rate to 8.595177052156611e-05\n",
      "epoch: 12 MSE loss: 0.1519 MSE valid loss: 0.2686\n",
      "Best score! Weights of the model are updated!\n",
      "Updating learning rate to 8.282869524032146e-05\n",
      "epoch: 13 MSE loss: 0.1496 MSE valid loss: 0.2721\n",
      "Updating learning rate to 8.002304995805999e-05\n",
      "epoch: 14 MSE loss: 0.1475 MSE valid loss: 0.2734\n",
      "Updating learning rate to 7.748446592171796e-05\n",
      "epoch: 15 MSE loss: 0.1457 MSE valid loss: 0.2694\n",
      "Updating learning rate to 7.517309741553296e-05\n",
      "epoch: 16 MSE loss: 0.1442 MSE valid loss: 0.2660\n",
      "Best score! Weights of the model are updated!\n",
      "Updating learning rate to 7.305695402335044e-05\n",
      "epoch: 17 MSE loss: 0.1427 MSE valid loss: 0.2738\n",
      "Updating learning rate to 7.111001549857179e-05\n",
      "epoch: 18 MSE loss: 0.1412 MSE valid loss: 0.2631\n",
      "Best score! Weights of the model are updated!\n",
      "Updating learning rate to 6.931087162517846e-05\n",
      "epoch: 19 MSE loss: 0.1406 MSE valid loss: 0.2628\n",
      "Best score! Weights of the model are updated!\n",
      "Updating learning rate to 6.76417225936176e-05\n",
      "epoch: 20 MSE loss: 0.1394 MSE valid loss: 0.2626\n",
      "Best score! Weights of the model are updated!\n",
      "Updating learning rate to 6.608763214317867e-05\n",
      "epoch: 21 MSE loss: 0.1376 MSE valid loss: 0.2613\n",
      "Best score! Weights of the model are updated!\n",
      "Updating learning rate to 6.46359612493774e-05\n",
      "epoch: 22 MSE loss: 0.1369 MSE valid loss: 0.2613\n",
      "Best score! Weights of the model are updated!\n",
      "Updating learning rate to 6.327593294406921e-05\n",
      "epoch: 23 MSE loss: 0.1357 MSE valid loss: 0.2574\n",
      "Best score! Weights of the model are updated!\n",
      "Updating learning rate to 6.199829383043036e-05\n",
      "epoch: 24 MSE loss: 0.1349 MSE valid loss: 0.2596\n",
      "Updating learning rate to 6.079504788572469e-05\n",
      "epoch: 25 MSE loss: 0.1352 MSE valid loss: 0.2634\n",
      "Updating learning rate to 5.965924498791846e-05\n",
      "epoch: 26 MSE loss: 0.1339 MSE valid loss: 0.2644\n",
      "Updating learning rate to 5.8584811349126495e-05\n",
      "epoch: 27 MSE loss: 0.1322 MSE valid loss: 0.2573\n",
      "Best score! Weights of the model are updated!\n",
      "Updating learning rate to 5.7566412382313e-05\n",
      "epoch: 28 MSE loss: 0.1323 MSE valid loss: 0.2641\n",
      "Updating learning rate to 5.659934091583233e-05\n",
      "epoch: 29 MSE loss: 0.1313 MSE valid loss: 0.2535\n",
      "Best score! Weights of the model are updated!\n",
      "Updating learning rate to 5.567942539842175e-05\n",
      "epoch: 30 MSE loss: 0.1308 MSE valid loss: 0.2617\n",
      "Updating learning rate to 5.4802954002676805e-05\n",
      "epoch: 31 MSE loss: 0.1306 MSE valid loss: 0.2614\n",
      "Updating learning rate to 5.39666114720432e-05\n",
      "epoch: 32 MSE loss: 0.1294 MSE valid loss: 0.2558\n",
      "Updating learning rate to 5.316742625738918e-05\n",
      "epoch: 33 MSE loss: 0.1289 MSE valid loss: 0.2600\n",
      "Updating learning rate to 5.240272601878982e-05\n",
      "epoch: 34 MSE loss: 0.1289 MSE valid loss: 0.2672\n",
      "Updating learning rate to 5.1670099971819504e-05\n",
      "epoch: 35 MSE loss: 0.1278 MSE valid loss: 0.2564\n",
      "Updating learning rate to 5.096736686795811e-05\n",
      "epoch: 36 MSE loss: 0.1276 MSE valid loss: 0.2641\n",
      "Updating learning rate to 5.0292547639160534e-05\n",
      "epoch: 37 MSE loss: 0.1266 MSE valid loss: 0.2563\n",
      "Updating learning rate to 4.96438419243461e-05\n",
      "epoch: 38 MSE loss: 0.1274 MSE valid loss: 0.2585\n",
      "Updating learning rate to 4.901960784313725e-05\n",
      "epoch: 39 MSE loss: 0.1259 MSE valid loss: 0.2549\n",
      "Updating learning rate to 4.841834449897069e-05\n",
      "epoch: 40 MSE loss: 0.1261 MSE valid loss: 0.2626\n",
      "Updating learning rate to 4.783867678672252e-05\n",
      "epoch: 41 MSE loss: 0.1258 MSE valid loss: 0.2586\n",
      "Updating learning rate to 4.727934215451461e-05\n",
      "epoch: 42 MSE loss: 0.1249 MSE valid loss: 0.2525\n",
      "Best score! Weights of the model are updated!\n",
      "Updating learning rate to 4.6739179029417445e-05\n",
      "epoch: 43 MSE loss: 0.1241 MSE valid loss: 0.2639\n",
      "Updating learning rate to 4.621711666540848e-05\n",
      "epoch: 44 MSE loss: 0.1241 MSE valid loss: 0.2637\n",
      "Updating learning rate to 4.571216621155238e-05\n",
      "epoch: 45 MSE loss: 0.1239 MSE valid loss: 0.2606\n",
      "Updating learning rate to 4.522341283077635e-05\n",
      "epoch: 46 MSE loss: 0.1233 MSE valid loss: 0.2638\n",
      "Updating learning rate to 4.4750008726252546e-05\n",
      "epoch: 47 MSE loss: 0.1227 MSE valid loss: 0.2580\n",
      "Updating learning rate to 4.4291166954394496e-05\n",
      "epoch: 48 MSE loss: 0.1230 MSE valid loss: 0.2633\n",
      "Updating learning rate to 4.384615592171157e-05\n",
      "epoch: 49 MSE loss: 0.1241 MSE valid loss: 0.2619\n",
      "Updating learning rate to 4.341429447794924e-05\n",
      "epoch: 50 MSE loss: 0.1219 MSE valid loss: 0.2543\n",
      "Updating learning rate to 4.299494753063186e-05\n",
      "epoch: 51 MSE loss: 0.1222 MSE valid loss: 0.2636\n",
      "Updating learning rate to 4.2587522116770666e-05\n",
      "epoch: 52 MSE loss: 0.1214 MSE valid loss: 0.2593\n",
      "Updating learning rate to 4.219146387646126e-05\n",
      "epoch: 53 MSE loss: 0.1209 MSE valid loss: 0.2665\n",
      "Updating learning rate to 4.1806253880665696e-05\n",
      "epoch: 54 MSE loss: 0.1212 MSE valid loss: 0.2663\n",
      "Updating learning rate to 4.143140577189121e-05\n",
      "epoch: 55 MSE loss: 0.1203 MSE valid loss: 0.2669\n",
      "Updating learning rate to 4.106646318193315e-05\n",
      "epoch: 56 MSE loss: 0.1201 MSE valid loss: 0.2652\n",
      "Updating learning rate to 4.071099739550263e-05\n",
      "epoch: 57 MSE loss: 0.1200 MSE valid loss: 0.2712\n",
      "Updating learning rate to 4.0364605232539164e-05\n",
      "epoch: 58 MSE loss: 0.1195 MSE valid loss: 0.2622\n",
      "Updating learning rate to 4.0026907125422175e-05\n",
      "epoch: 59 MSE loss: 0.1196 MSE valid loss: 0.2734\n",
      "Updating learning rate to 3.969754537023144e-05\n",
      "epoch: 60 MSE loss: 0.1191 MSE valid loss: 0.2688\n",
      "Updating learning rate to 3.937618253373847e-05\n",
      "epoch: 61 MSE loss: 0.1186 MSE valid loss: 0.2667\n",
      "Updating learning rate to 3.90625e-05\n",
      "epoch: 62 MSE loss: 0.1186 MSE valid loss: 0.2683\n",
      "Updating learning rate to 3.875619664232189e-05\n",
      "epoch: 63 MSE loss: 0.1183 MSE valid loss: 0.2635\n",
      "Updating learning rate to 3.845698760800996e-05\n",
      "epoch: 64 MSE loss: 0.1183 MSE valid loss: 0.2684\n",
      "Updating learning rate to 3.816460320475954e-05\n",
      "epoch: 65 MSE loss: 0.1183 MSE valid loss: 0.2675\n",
      "Updating learning rate to 3.787878787878788e-05\n",
      "epoch: 66 MSE loss: 0.1172 MSE valid loss: 0.2647\n",
      "Updating learning rate to 3.759929927590882e-05\n",
      "epoch: 67 MSE loss: 0.1179 MSE valid loss: 0.2736\n",
      "Updating learning rate to 3.732590737770921e-05\n",
      "epoch: 68 MSE loss: 0.1171 MSE valid loss: 0.2713\n",
      "Updating learning rate to 3.7058393705829335e-05\n",
      "epoch: 69 MSE loss: 0.1173 MSE valid loss: 0.2692\n",
      "Updating learning rate to 3.679655058809149e-05\n",
      "epoch: 70 MSE loss: 0.1166 MSE valid loss: 0.2693\n",
      "Updating learning rate to 3.654018048087443e-05\n",
      "epoch: 71 MSE loss: 0.1163 MSE valid loss: 0.2752\n",
      "Updating learning rate to 3.6289095342709304e-05\n",
      "epoch: 72 MSE loss: 0.1162 MSE valid loss: 0.2709\n",
      "Updating learning rate to 3.604311605458291e-05\n",
      "epoch: 73 MSE loss: 0.1158 MSE valid loss: 0.2753\n",
      "Updating learning rate to 3.58020718828878e-05\n",
      "epoch: 74 MSE loss: 0.1165 MSE valid loss: 0.2645\n",
      "Updating learning rate to 3.5565799981359994e-05\n",
      "epoch: 75 MSE loss: 0.1163 MSE valid loss: 0.2680\n",
      "Updating learning rate to 3.5334144928703015e-05\n",
      "epoch: 76 MSE loss: 0.1154 MSE valid loss: 0.2746\n",
      "Updating learning rate to 3.510695829891526e-05\n",
      "epoch: 77 MSE loss: 0.1154 MSE valid loss: 0.2740\n",
      "Updating learning rate to 3.488409826162172e-05\n",
      "epoch: 78 MSE loss: 0.1153 MSE valid loss: 0.2690\n",
      "Updating learning rate to 3.4665429209964995e-05\n",
      "epoch: 79 MSE loss: 0.1146 MSE valid loss: 0.2710\n",
      "Updating learning rate to 3.445082141383733e-05\n",
      "epoch: 80 MSE loss: 0.1158 MSE valid loss: 0.2714\n",
      "Updating learning rate to 3.424015069643934e-05\n",
      "epoch: 81 MSE loss: 0.1148 MSE valid loss: 0.2837\n",
      "Updating learning rate to 3.4033298132333136e-05\n",
      "epoch: 82 MSE loss: 0.1146 MSE valid loss: 0.2697\n",
      "Updating learning rate to 3.383014976532195e-05\n",
      "epoch: 83 MSE loss: 0.1150 MSE valid loss: 0.2698\n",
      "Updating learning rate to 3.3630596344635906e-05\n",
      "epoch: 84 MSE loss: 0.1140 MSE valid loss: 0.2796\n",
      "Updating learning rate to 3.343453307803635e-05\n",
      "epoch: 85 MSE loss: 0.1137 MSE valid loss: 0.2716\n",
      "Updating learning rate to 3.3241859400571366e-05\n",
      "epoch: 86 MSE loss: 0.1139 MSE valid loss: 0.2789\n",
      "Updating learning rate to 3.305247875782325e-05\n",
      "epoch: 87 MSE loss: 0.1138 MSE valid loss: 0.2806\n",
      "Updating learning rate to 3.286629840258662e-05\n",
      "epoch: 88 MSE loss: 0.1137 MSE valid loss: 0.2765\n",
      "Updating learning rate to 3.268322920400467e-05\n",
      "epoch: 89 MSE loss: 0.1135 MSE valid loss: 0.2777\n",
      "Updating learning rate to 3.250318546827149e-05\n",
      "epoch: 90 MSE loss: 0.1139 MSE valid loss: 0.2777\n",
      "Updating learning rate to 3.232608477008077e-05\n",
      "epoch: 91 MSE loss: 0.1135 MSE valid loss: 0.2813\n",
      "Updating learning rate to 3.21518477940684e-05\n",
      "epoch: 92 MSE loss: 0.1134 MSE valid loss: 0.2823\n",
      "Updating learning rate to 3.198039818555568e-05\n",
      "epoch: 93 MSE loss: 0.1135 MSE valid loss: 0.2759\n",
      "Updating learning rate to 3.181166240995547e-05\n",
      "epoch: 94 MSE loss: 0.1135 MSE valid loss: 0.2860\n",
      "Updating learning rate to 3.1645569620253167e-05\n",
      "epoch: 95 MSE loss: 0.1135 MSE valid loss: 0.2779\n",
      "Updating learning rate to 3.148205153202001e-05\n",
      "epoch: 96 MSE loss: 0.1126 MSE valid loss: 0.2754\n",
      "Updating learning rate to 3.1321042305458135e-05\n",
      "epoch: 97 MSE loss: 0.1137 MSE valid loss: 0.2793\n",
      "Updating learning rate to 3.1162478434014266e-05\n",
      "epoch: 98 MSE loss: 0.1125 MSE valid loss: 0.2818\n",
      "Updating learning rate to 3.100629863913435e-05\n",
      "epoch: 99 MSE loss: 0.1123 MSE valid loss: 0.2903\n",
      "Updating learning rate to 4.811252243246882e-08\n",
      "Updating learning rate to 3.175426480542942e-06\n",
      "epoch: 0 MSE loss: 0.2400 MSE valid loss: 0.1334\n",
      "Best score! Weights of the model are updated!\n",
      "Updating learning rate to 6.302740438653415e-06\n",
      "epoch: 1 MSE loss: 0.2359 MSE valid loss: 0.1290\n",
      "Best score! Weights of the model are updated!\n",
      "Updating learning rate to 9.430054396763887e-06\n",
      "epoch: 2 MSE loss: 0.2276 MSE valid loss: 0.1232\n",
      "Best score! Weights of the model are updated!\n",
      "Updating learning rate to 1.2557368354874362e-05\n",
      "epoch: 3 MSE loss: 0.2175 MSE valid loss: 0.1185\n",
      "Best score! Weights of the model are updated!\n",
      "Updating learning rate to 1.3846219390542782e-05\n",
      "epoch: 4 MSE loss: 0.2097 MSE valid loss: 0.1145\n",
      "Best score! Weights of the model are updated!\n",
      "Updating learning rate to 1.264304343560434e-05\n",
      "epoch: 5 MSE loss: 0.2021 MSE valid loss: 0.1109\n",
      "Best score! Weights of the model are updated!\n",
      "Updating learning rate to 1.1707322644771175e-05\n",
      "epoch: 6 MSE loss: 0.1963 MSE valid loss: 0.1083\n",
      "Best score! Weights of the model are updated!\n",
      "Updating learning rate to 1.0952698858458087e-05\n",
      "epoch: 7 MSE loss: 0.1895 MSE valid loss: 0.1063\n",
      "Best score! Weights of the model are updated!\n",
      "Updating learning rate to 1.0327404809650346e-05\n",
      "epoch: 8 MSE loss: 0.1865 MSE valid loss: 0.1049\n",
      "Best score! Weights of the model are updated!\n",
      "Updating learning rate to 9.798272520870257e-06\n",
      "epoch: 9 MSE loss: 0.1812 MSE valid loss: 0.1039\n",
      "Best score! Weights of the model are updated!\n",
      "Updating learning rate to 9.342938659399199e-06\n",
      "epoch: 10 MSE loss: 0.1778 MSE valid loss: 0.1032\n",
      "Best score! Weights of the model are updated!\n",
      "Updating learning rate to 8.945703337056415e-06\n",
      "epoch: 11 MSE loss: 0.1735 MSE valid loss: 0.1027\n",
      "Best score! Weights of the model are updated!\n",
      "Updating learning rate to 8.595177052156612e-06\n",
      "epoch: 12 MSE loss: 0.1705 MSE valid loss: 0.1026\n",
      "Best score! Weights of the model are updated!\n",
      "Updating learning rate to 8.282869524032146e-06\n",
      "epoch: 13 MSE loss: 0.1679 MSE valid loss: 0.1026\n",
      "Best score! Weights of the model are updated!\n",
      "Updating learning rate to 8.002304995806e-06\n",
      "epoch: 14 MSE loss: 0.1658 MSE valid loss: 0.1029\n",
      "Updating learning rate to 7.748446592171797e-06\n",
      "epoch: 15 MSE loss: 0.1634 MSE valid loss: 0.1031\n",
      "Updating learning rate to 7.517309741553296e-06\n",
      "epoch: 16 MSE loss: 0.1609 MSE valid loss: 0.1034\n",
      "Updating learning rate to 7.305695402335044e-06\n",
      "epoch: 17 MSE loss: 0.1591 MSE valid loss: 0.1039\n",
      "Updating learning rate to 7.1110015498571785e-06\n",
      "epoch: 18 MSE loss: 0.1583 MSE valid loss: 0.1043\n",
      "Updating learning rate to 6.931087162517846e-06\n",
      "epoch: 19 MSE loss: 0.1567 MSE valid loss: 0.1049\n",
      "Updating learning rate to 6.76417225936176e-06\n",
      "epoch: 20 MSE loss: 0.1552 MSE valid loss: 0.1054\n",
      "Updating learning rate to 6.608763214317868e-06\n",
      "epoch: 21 MSE loss: 0.1538 MSE valid loss: 0.1059\n",
      "Updating learning rate to 6.463596124937739e-06\n",
      "epoch: 22 MSE loss: 0.1531 MSE valid loss: 0.1065\n",
      "Updating learning rate to 6.327593294406922e-06\n",
      "epoch: 23 MSE loss: 0.1517 MSE valid loss: 0.1070\n",
      "Updating learning rate to 6.199829383043036e-06\n",
      "epoch: 24 MSE loss: 0.1501 MSE valid loss: 0.1075\n",
      "Updating learning rate to 6.079504788572469e-06\n",
      "epoch: 25 MSE loss: 0.1497 MSE valid loss: 0.1080\n",
      "Updating learning rate to 5.965924498791846e-06\n",
      "epoch: 26 MSE loss: 0.1492 MSE valid loss: 0.1086\n",
      "Updating learning rate to 5.8584811349126495e-06\n",
      "epoch: 27 MSE loss: 0.1480 MSE valid loss: 0.1092\n",
      "Updating learning rate to 5.7566412382313e-06\n",
      "epoch: 28 MSE loss: 0.1468 MSE valid loss: 0.1096\n",
      "Updating learning rate to 5.659934091583234e-06\n",
      "epoch: 29 MSE loss: 0.1461 MSE valid loss: 0.1103\n",
      "Updating learning rate to 5.567942539842175e-06\n",
      "epoch: 30 MSE loss: 0.1457 MSE valid loss: 0.1108\n",
      "Updating learning rate to 5.48029540026768e-06\n",
      "epoch: 31 MSE loss: 0.1444 MSE valid loss: 0.1112\n",
      "Updating learning rate to 5.39666114720432e-06\n",
      "epoch: 32 MSE loss: 0.1439 MSE valid loss: 0.1116\n",
      "Updating learning rate to 5.316742625738919e-06\n",
      "epoch: 33 MSE loss: 0.1431 MSE valid loss: 0.1120\n",
      "Updating learning rate to 5.240272601878983e-06\n",
      "epoch: 34 MSE loss: 0.1421 MSE valid loss: 0.1126\n",
      "Updating learning rate to 5.16700999718195e-06\n",
      "epoch: 35 MSE loss: 0.1421 MSE valid loss: 0.1132\n",
      "Updating learning rate to 5.096736686795811e-06\n",
      "epoch: 36 MSE loss: 0.1411 MSE valid loss: 0.1136\n",
      "Updating learning rate to 5.029254763916053e-06\n",
      "epoch: 37 MSE loss: 0.1406 MSE valid loss: 0.1137\n",
      "Updating learning rate to 4.964384192434611e-06\n",
      "epoch: 38 MSE loss: 0.1405 MSE valid loss: 0.1144\n",
      "Updating learning rate to 4.901960784313726e-06\n",
      "epoch: 39 MSE loss: 0.1391 MSE valid loss: 0.1148\n",
      "Updating learning rate to 4.841834449897069e-06\n",
      "epoch: 40 MSE loss: 0.1386 MSE valid loss: 0.1151\n",
      "Updating learning rate to 4.783867678672252e-06\n",
      "epoch: 41 MSE loss: 0.1393 MSE valid loss: 0.1154\n",
      "Updating learning rate to 4.727934215451461e-06\n",
      "epoch: 42 MSE loss: 0.1393 MSE valid loss: 0.1158\n",
      "Updating learning rate to 4.673917902941745e-06\n",
      "epoch: 43 MSE loss: 0.1377 MSE valid loss: 0.1158\n",
      "Updating learning rate to 4.621711666540848e-06\n",
      "epoch: 44 MSE loss: 0.1379 MSE valid loss: 0.1166\n",
      "Updating learning rate to 4.571216621155238e-06\n",
      "epoch: 45 MSE loss: 0.1376 MSE valid loss: 0.1167\n",
      "Updating learning rate to 4.5223412830776355e-06\n",
      "epoch: 46 MSE loss: 0.1368 MSE valid loss: 0.1171\n",
      "Updating learning rate to 4.475000872625255e-06\n",
      "epoch: 47 MSE loss: 0.1361 MSE valid loss: 0.1172\n",
      "Updating learning rate to 4.42911669543945e-06\n",
      "epoch: 48 MSE loss: 0.1361 MSE valid loss: 0.1178\n",
      "Updating learning rate to 4.3846155921711576e-06\n",
      "epoch: 49 MSE loss: 0.1352 MSE valid loss: 0.1181\n",
      "Updating learning rate to 4.341429447794925e-06\n",
      "epoch: 50 MSE loss: 0.1358 MSE valid loss: 0.1183\n",
      "Updating learning rate to 4.299494753063186e-06\n",
      "epoch: 51 MSE loss: 0.1349 MSE valid loss: 0.1181\n",
      "Updating learning rate to 4.258752211677067e-06\n",
      "epoch: 52 MSE loss: 0.1343 MSE valid loss: 0.1187\n",
      "Updating learning rate to 4.219146387646126e-06\n",
      "epoch: 53 MSE loss: 0.1338 MSE valid loss: 0.1188\n",
      "Updating learning rate to 4.18062538806657e-06\n",
      "epoch: 54 MSE loss: 0.1336 MSE valid loss: 0.1188\n",
      "Updating learning rate to 4.143140577189122e-06\n",
      "epoch: 55 MSE loss: 0.1344 MSE valid loss: 0.1191\n",
      "Updating learning rate to 4.106646318193315e-06\n",
      "epoch: 56 MSE loss: 0.1329 MSE valid loss: 0.1193\n",
      "Updating learning rate to 4.071099739550263e-06\n",
      "epoch: 57 MSE loss: 0.1334 MSE valid loss: 0.1192\n",
      "Updating learning rate to 4.036460523253916e-06\n",
      "epoch: 58 MSE loss: 0.1318 MSE valid loss: 0.1200\n",
      "Updating learning rate to 4.002690712542218e-06\n",
      "epoch: 59 MSE loss: 0.1316 MSE valid loss: 0.1200\n",
      "Updating learning rate to 3.969754537023144e-06\n",
      "epoch: 60 MSE loss: 0.1312 MSE valid loss: 0.1205\n",
      "Updating learning rate to 3.937618253373847e-06\n",
      "epoch: 61 MSE loss: 0.1312 MSE valid loss: 0.1200\n",
      "Updating learning rate to 3.90625e-06\n",
      "epoch: 62 MSE loss: 0.1313 MSE valid loss: 0.1204\n",
      "Updating learning rate to 3.8756196642321895e-06\n",
      "epoch: 63 MSE loss: 0.1307 MSE valid loss: 0.1210\n",
      "Updating learning rate to 3.845698760800996e-06\n",
      "epoch: 64 MSE loss: 0.1308 MSE valid loss: 0.1209\n",
      "Updating learning rate to 3.816460320475954e-06\n",
      "epoch: 65 MSE loss: 0.1303 MSE valid loss: 0.1210\n",
      "Updating learning rate to 3.7878787878787882e-06\n",
      "epoch: 66 MSE loss: 0.1294 MSE valid loss: 0.1210\n",
      "Updating learning rate to 3.759929927590882e-06\n",
      "epoch: 67 MSE loss: 0.1295 MSE valid loss: 0.1213\n",
      "Updating learning rate to 3.732590737770921e-06\n",
      "epoch: 68 MSE loss: 0.1294 MSE valid loss: 0.1213\n",
      "Updating learning rate to 3.7058393705829336e-06\n",
      "epoch: 69 MSE loss: 0.1289 MSE valid loss: 0.1212\n",
      "Updating learning rate to 3.6796550588091485e-06\n",
      "epoch: 70 MSE loss: 0.1292 MSE valid loss: 0.1207\n",
      "Updating learning rate to 3.6540180480874438e-06\n",
      "epoch: 71 MSE loss: 0.1284 MSE valid loss: 0.1213\n",
      "Updating learning rate to 3.6289095342709304e-06\n",
      "epoch: 72 MSE loss: 0.1284 MSE valid loss: 0.1212\n",
      "Updating learning rate to 3.604311605458291e-06\n",
      "epoch: 73 MSE loss: 0.1282 MSE valid loss: 0.1216\n",
      "Updating learning rate to 3.58020718828878e-06\n",
      "epoch: 74 MSE loss: 0.1283 MSE valid loss: 0.1217\n",
      "Updating learning rate to 3.5565799981359997e-06\n",
      "epoch: 75 MSE loss: 0.1268 MSE valid loss: 0.1217\n",
      "Updating learning rate to 3.5334144928703013e-06\n",
      "epoch: 76 MSE loss: 0.1266 MSE valid loss: 0.1217\n",
      "Updating learning rate to 3.5106958298915256e-06\n",
      "epoch: 77 MSE loss: 0.1265 MSE valid loss: 0.1220\n",
      "Updating learning rate to 3.4884098261621723e-06\n",
      "epoch: 78 MSE loss: 0.1260 MSE valid loss: 0.1215\n",
      "Updating learning rate to 3.466542920996499e-06\n",
      "epoch: 79 MSE loss: 0.1259 MSE valid loss: 0.1219\n",
      "Updating learning rate to 3.4450821413837328e-06\n",
      "epoch: 80 MSE loss: 0.1262 MSE valid loss: 0.1221\n",
      "Updating learning rate to 3.4240150696439336e-06\n",
      "epoch: 81 MSE loss: 0.1256 MSE valid loss: 0.1218\n",
      "Updating learning rate to 3.4033298132333132e-06\n",
      "epoch: 82 MSE loss: 0.1260 MSE valid loss: 0.1224\n",
      "Updating learning rate to 3.383014976532195e-06\n",
      "epoch: 83 MSE loss: 0.1258 MSE valid loss: 0.1225\n",
      "Updating learning rate to 3.3630596344635904e-06\n",
      "epoch: 84 MSE loss: 0.1255 MSE valid loss: 0.1222\n",
      "Updating learning rate to 3.3434533078036348e-06\n",
      "epoch: 85 MSE loss: 0.1255 MSE valid loss: 0.1228\n",
      "Updating learning rate to 3.3241859400571367e-06\n",
      "epoch: 86 MSE loss: 0.1250 MSE valid loss: 0.1224\n",
      "Updating learning rate to 3.305247875782325e-06\n",
      "epoch: 87 MSE loss: 0.1239 MSE valid loss: 0.1228\n",
      "Updating learning rate to 3.2866298402586612e-06\n",
      "epoch: 88 MSE loss: 0.1245 MSE valid loss: 0.1228\n",
      "Updating learning rate to 3.2683229204004673e-06\n",
      "epoch: 89 MSE loss: 0.1246 MSE valid loss: 0.1226\n",
      "Updating learning rate to 3.2503185468271486e-06\n",
      "epoch: 90 MSE loss: 0.1241 MSE valid loss: 0.1230\n",
      "Updating learning rate to 3.232608477008077e-06\n",
      "epoch: 91 MSE loss: 0.1244 MSE valid loss: 0.1229\n",
      "Updating learning rate to 3.2151847794068398e-06\n",
      "epoch: 92 MSE loss: 0.1243 MSE valid loss: 0.1230\n",
      "Updating learning rate to 3.1980398185555676e-06\n",
      "epoch: 93 MSE loss: 0.1233 MSE valid loss: 0.1231\n",
      "Updating learning rate to 3.1811662409955475e-06\n",
      "epoch: 94 MSE loss: 0.1231 MSE valid loss: 0.1233\n",
      "Updating learning rate to 3.1645569620253167e-06\n",
      "epoch: 95 MSE loss: 0.1231 MSE valid loss: 0.1228\n",
      "Updating learning rate to 3.1482051532020014e-06\n",
      "epoch: 96 MSE loss: 0.1224 MSE valid loss: 0.1235\n",
      "Updating learning rate to 3.1321042305458136e-06\n",
      "epoch: 97 MSE loss: 0.1223 MSE valid loss: 0.1228\n",
      "Updating learning rate to 3.116247843401426e-06\n",
      "epoch: 98 MSE loss: 0.1225 MSE valid loss: 0.1235\n",
      "Updating learning rate to 3.1006298639134353e-06\n",
      "epoch: 99 MSE loss: 0.1220 MSE valid loss: 0.1234\n"
     ]
    }
   ],
   "source": [
    "Linear_train_history, Linear_valid_history = Linear_learner.train()\n",
    "DLinear_train_history, DLinear_valid_history = DLinear_learner.train()\n",
    "patchtst_train_history, patchtst_valid_history = patchtst_learner.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE test loss: 0.0943\n"
     ]
    }
   ],
   "source": [
    "Linear_learner.test()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE test loss: 0.1087\n"
     ]
    }
   ],
   "source": [
    "DLinear_learner.test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE test loss: 0.0621\n"
     ]
    }
   ],
   "source": [
    "patchtst_learner.test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x500 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig, ax = plt.subplots(1, 1, figsize=(10, 5))\n",
    "ax.plot(np.arange(1, Linear_learner.epochs+1), Linear_valid_history, label=\"Linear\")\n",
    "ax.plot(np.arange(1, DLinear_learner.epochs+1), DLinear_valid_history, label=\"DLinear\")\n",
    "ax.plot(np.arange(1, patchtst_learner.epochs+1), patchtst_valid_history, label=\"PatchTST\")\n",
    "ax.set_xlabel(\"epochs\")\n",
    "ax.set_ylabel(\"MSE Error\")\n",
    "ax.set_ylim(0, 1.5)\n",
    "ax.legend()\n",
    "fig.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "demand_forecast_DNN-3mvUAJM5",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
